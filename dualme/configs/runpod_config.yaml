runpod:
  # Configurazione del pod
  template: "PyTorch 2.1.0"
  gpu: "NVIDIA A4000"  # o superiore
  volume: "/workspace"
  port: 7860
  
  # Configurazione persistente
  persistent_storage:
    enabled: true
    mount_path: "/workspace"
    size_gb: 100  # Dimensione del volume persistente
    backup:
      enabled: true
      frequency: "daily"
      retention: 7  # giorni
    
  # Configurazione delle risorse
  resources:
    gpu_memory: "16GB"
    cpu_cores: 8
    ram: "32GB"
    swap: "16GB"
    
  # Configurazione del container
  container:
    image: "pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime"
    working_dir: "/workspace"
    environment:
      - name: "PYTHONUNBUFFERED"
        value: "1"
      - name: "CUDA_VISIBLE_DEVICES"
        value: "0"
      - name: "TORCH_CUDA_ARCH_LIST"
        value: "7.5;8.0;8.6"  # Ottimizzazione per A4000
      - name: "OMP_NUM_THREADS"
        value: "8"
      - name: "MKL_NUM_THREADS"
        value: "8"
        
  # Configurazione del deployment
  deployment:
    auto_restart: true
    max_restarts: 3
    restart_delay: 60
    health_check:
      enabled: true
      interval: 30
      timeout: 10
      retries: 3
    
  # Configurazione del monitoraggio
  monitoring:
    enabled: true
    metrics:
      - gpu_usage
      - memory_usage
      - disk_usage
      - temperature
      - power_usage
    alerts:
      - type: "disk_space"
        threshold: 90
      - type: "gpu_memory"
        threshold: 95
      - type: "temperature"
        threshold: 85
      - type: "power_usage"
        threshold: 250  # Watt
        
  # Configurazione delle performance
  performance:
    cuda_optimization: true
    mixed_precision: true
    batch_size: 4
    num_workers: 4
    pin_memory: true
    
  # Configurazione della sicurezza
  security:
    ssl_enabled: true
    authentication: true
    rate_limiting:
      enabled: true
      requests_per_minute: 60 